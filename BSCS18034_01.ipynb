{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bba7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "input_dim = 784\n",
    "epochs=10\n",
    "train_size=20000\n",
    "validation_size=2000\n",
    "test_size=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af1cbf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"MNIST_Data/\"\n",
    "\n",
    "# declaring transform func i.e normalizing and resizing\n",
    "train_transforms = transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transforms = transforms.Compose([transforms.Resize((28,28)),transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "# Loading dataset using ImageFolder method. Pass transforms in here.\n",
    "train_data = datasets.ImageFolder(path + '/train', transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(path + '/test', transform=test_transforms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "73215386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 60000\n",
      "    Root location: MNIST_Data//train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(28, 28), interpolation=PIL.Image.BILINEAR)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6feadbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.random.choice(len(train_data), train_size+validation_size, replace=False)\n",
    "test_index = np.random.choice(len(test_data), test_size, replace = False)\n",
    "train_index, validation_index = torch.utils.data.random_split(train_index,[train_size, validation_size])\n",
    "\n",
    "training_data = torch.utils.data.Subset(dataset=train_data, indices=train_index)\n",
    "validation_data = torch.utils.data.Subset(dataset=train_data, indices=validation_index)\n",
    "testing_data = torch.utils.data.Subset(dataset=test_data, indices=test_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5326ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testing_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7ea59b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000012F56E2A280>\n"
     ]
    }
   ],
   "source": [
    "print(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7e0abd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        neurons_per_layer = [100,50,10]\n",
    "        dropout=0.2\n",
    "        self.ra1 = nn.Linear(input_dim*3, neurons_per_layer[0])\n",
    "        self.ra2 = nn.Linear(neurons_per_layer[0], neurons_per_layer[1])\n",
    "        self.ra3 = nn.Linear(neurons_per_layer[1], neurons_per_layer[2])\n",
    "        self.dropout1 = nn.Dropout2d(dropout)\n",
    "        self.dropout2 = nn.Dropout2d(dropout)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.ra1(x))\n",
    "        x = F.relu(self.ra2(x))\n",
    "        x = self.ra3(x)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40023a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2db2d7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (ra1): Linear(in_features=2352, out_features=100, bias=True)\n",
       "  (ra2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (ra3): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (dropout1): Dropout2d(p=0.2, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "learning_rate=0.0005\n",
    "epochs=10\n",
    "\n",
    "# this statement tell our code if there gpu available on our machine or not.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14c664b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d2eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "423645e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3130\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "count =0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    trainAccuracy=0\n",
    "    trainLoss=0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        data = data.view(-1, input_dim*3)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = model(data)\n",
    "    #     print('Targets',target)\n",
    "        loss = loss_function(net_out, target)  # loss_func is criterion\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainAccuracy += (torch.argmax(net_out, dim=1) == target).float().mean().item()\n",
    "        trainLoss += loss.item()\n",
    "        count+=1\n",
    "    totalTrainLoss=[]\n",
    "    totalTrainAccuracy=[]\n",
    "    totalTrainLoss.append(trainLoss/len(train_loader))\n",
    "    totalTrainAccuracy.append(trainAccuracy/len(train_loader))\n",
    "print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3eda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bcf77d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy=0\n",
    "Loss=0\n",
    "for batch_idx, (data, target) in enumerate(validation_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data = data.view(-1, input_dim*3)\n",
    "    optimizer.zero_grad()\n",
    "    net_out = model(data)\n",
    "    loss = loss_function(net_out, target)  # loss_func is criterion\n",
    "    Accuracy += (torch.argmax(net_out, dim=1) == target).float().mean().item()\n",
    "    Loss += loss.item()\n",
    "    count+=1\n",
    "totalValidationLoss=Loss/len(validation_loader)\n",
    "totalValidationAccuracy=Accuracy/len(validation_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "887a3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8f8ad39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3319\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Accuracy=0\n",
    "Loss=0\n",
    "for batch_idx, (data, target) in enumerate(test_loader):\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    data = data.view(-1, input_dim*3)\n",
    "    optimizer.zero_grad()\n",
    "    net_out = model(data)\n",
    "    loss = loss_function(net_out, target)  # loss_func is criterion\n",
    "    Accuracy += (torch.argmax(net_out, dim=1) == target).float().mean().item()\n",
    "    Loss += loss.item()\n",
    "    count+=1\n",
    "totalTestLoss=Loss/len(test_loader)\n",
    "totalTestAccuracy=Accuracy/len(test_loader)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "11349d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"chakkidumba.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c7ab110b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"chakkidumba.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1330eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(totalTrainAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "732c3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9329073482428115]\n"
     ]
    }
   ],
   "source": [
    "print((totalTrainAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e9075bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92333984375\n"
     ]
    }
   ],
   "source": [
    "print(totalValidationAccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "169de210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9308320063694268\n"
     ]
    }
   ],
   "source": [
    "print((totalTestAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025725d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
